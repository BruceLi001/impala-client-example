
// Copyright 2015 Cloudera Inc.
//
// Licensed under the Apache License, Version 2.0 (the "License");
// you may not use this file except in compliance with the License.
// You may obtain a copy of the License at
//
// http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing, software
// distributed under the License is distributed on an "AS IS" BASIS,
// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
// See the License for the specific language governing permissions and
// limitations under the License.
//
//
// THIS FILE IS AUTO GENERATED BY generated_error_codes.py DO NOT MODIFY
// IT BY HAND.
//

namespace cpp impala
namespace java com.cloudera.impala.thrift


enum TErrorCode {
  OK = 0,
  UNUSED = 1,
  GENERAL = 2,
  CANCELLED = 3,
  ANALYSIS_ERROR = 4,
  NOT_IMPLEMENTED_ERROR = 5,
  RUNTIME_ERROR = 6,
  MEM_LIMIT_EXCEEDED = 7,
  INTERNAL_ERROR = 8,
  RECOVERABLE_ERROR = 9,
  PARQUET_MULTIPLE_BLOCKS = 10,
  PARQUET_COLUMN_METADATA_INVALID = 11,
  PARQUET_HEADER_PAGE_SIZE_EXCEEDED = 12,
  PARQUET_HEADER_EOF = 13,
  PARQUET_GROUP_ROW_COUNT_ERROR = 14,
  PARQUET_GROUP_ROW_COUNT_OVERFLOW = 15,
  PARQUET_MISSING_PRECISION = 16,
  PARQUET_WRONG_PRECISION = 17,
  PARQUET_BAD_CONVERTED_TYPE = 18,
  PARQUET_INCOMPATIBLE_DECIMAL = 19,
  SEQUENCE_SCANNER_PARSE_ERROR = 20,
  SNAPPY_DECOMPRESS_INVALID_BLOCK_SIZE = 21,
  SNAPPY_DECOMPRESS_INVALID_COMPRESSED_LENGTH = 22,
  SNAPPY_DECOMPRESS_UNCOMPRESSED_LENGTH_FAILED = 23,
  SNAPPY_DECOMPRESS_RAW_UNCOMPRESS_FAILED = 24,
  SNAPPY_DECOMPRESS_DECOMPRESS_SIZE_INCORRECT = 25,
  HDFS_SCAN_NODE_UNKNOWN_DISK = 26,
  FRAGMENT_EXECUTOR = 27,
  PARTITIONED_HASH_JOIN_MAX_PARTITION_DEPTH = 28,
  PARTITIONED_AGG_MAX_PARTITION_DEPTH = 29,
  MISSING_BUILTIN = 30,
  RPC_GENERAL_ERROR = 31,
  RPC_TIMEOUT = 32,
  UDF_VERIFY_FAILED = 33,
  PARQUET_CORRUPT_VALUE = 34,
  AVRO_DECIMAL_RESOLUTION_ERROR = 35,
  AVRO_DECIMAL_METADATA_MISMATCH = 36,
  AVRO_SCHEMA_RESOLUTION_ERROR = 37,
  AVRO_SCHEMA_METADATA_MISMATCH = 38,
  AVRO_UNSUPPORTED_DEFAULT_VALUE = 39,
  AVRO_MISSING_FIELD = 40,
  AVRO_MISSING_DEFAULT = 41,
  AVRO_NULLABILITY_MISMATCH = 42,
  AVRO_NOT_A_RECORD = 43,
  PARQUET_DEF_LEVEL_ERROR = 44,
  PARQUET_NUM_COL_VALS_ERROR = 45,
  PARQUET_DICT_DECODE_FAILURE = 46,
  SSL_PASSWORD_CMD_FAILED = 47,
  SSL_CERTIFICATE_PATH_BLANK = 48,
  SSL_PRIVATE_KEY_PATH_BLANK = 49,
  SSL_CERTIFICATE_NOT_FOUND = 50,
  SSL_PRIVATE_KEY_NOT_FOUND = 51,
  SSL_SOCKET_CREATION_FAILED = 52,
  MEM_ALLOC_FAILED = 53,
  PARQUET_REP_LEVEL_ERROR = 54,
  PARQUET_UNRECOGNIZED_SCHEMA = 55,
  COLLECTION_ALLOC_FAILED = 56,
  TMP_DEVICE_BLACKLISTED = 57,
  TMP_FILE_BLACKLISTED = 58,
  RPC_CLIENT_CONNECT_FAILURE = 59,
  STALE_METADATA_FILE_TOO_SHORT = 60,
  PARQUET_BAD_VERSION_NUMBER = 61,
  SCANNER_INCOMPLETE_READ = 62,
  SCANNER_INVALID_READ = 63,
  AVRO_BAD_VERSION_HEADER = 64,
  UDF_MEM_LIMIT_EXCEEDED = 65,
  BTS_BLOCK_OVERFLOW = 66,
  COMPRESSED_FILE_MULTIPLE_BLOCKS = 67,
  COMPRESSED_FILE_BLOCK_CORRUPTED = 68,
  COMPRESSED_FILE_DECOMPRESSOR_ERROR = 69,
  COMPRESSED_FILE_DECOMPRESSOR_NO_PROGRESS = 70,
  COMPRESSED_FILE_TRUNCATED = 71,
  DATASTREAM_SENDER_TIMEOUT = 72,
  KUDU_IMPALA_TYPE_MISSING = 73,
  IMPALA_KUDU_TYPE_MISSING = 74,
  KUDU_NOT_SUPPORTED_ON_OS = 75,
  KUDU_NOT_ENABLED = 76,
  PARTITIONED_HASH_JOIN_REPARTITION_FAILS = 77,
  PARTITIONED_AGG_REPARTITION_FAILS = 78
}
const list<string> TErrorMessage = [
  // OK
  "",
  // UNUSED
  "<UNUSED>",
  // GENERAL
  "$0",
  // CANCELLED
  "$0",
  // ANALYSIS_ERROR
  "$0",
  // NOT_IMPLEMENTED_ERROR
  "$0",
  // RUNTIME_ERROR
  "$0",
  // MEM_LIMIT_EXCEEDED
  "$0",
  // INTERNAL_ERROR
  "$0",
  // RECOVERABLE_ERROR
  "$0",
  // PARQUET_MULTIPLE_BLOCKS
  "Parquet files should not be split into multiple hdfs-blocks. file=$0",
  // PARQUET_COLUMN_METADATA_INVALID
  "Column metadata states there are $0 values, but read $1 values from column $2. file=$3",
  // PARQUET_HEADER_PAGE_SIZE_EXCEEDED
  "(unused)",
  // PARQUET_HEADER_EOF
  "ParquetScanner: reached EOF while deserializing data page header. file=$0",
  // PARQUET_GROUP_ROW_COUNT_ERROR
  "Metadata states that in group $0($1) there are $2 rows, but $3 rows were read.",
  // PARQUET_GROUP_ROW_COUNT_OVERFLOW
  "(unused)",
  // PARQUET_MISSING_PRECISION
  "File '$0' column '$1' does not have the decimal precision set.",
  // PARQUET_WRONG_PRECISION
  "File '$0' column '$1' has a precision that does not match the table metadata  precision. File metadata precision: $2, table metadata precision: $3.",
  // PARQUET_BAD_CONVERTED_TYPE
  "File '$0' column '$1' does not have converted type set to DECIMAL",
  // PARQUET_INCOMPATIBLE_DECIMAL
  "File '$0' column '$1' contains decimal data but the table metadata has type $2",
  // SEQUENCE_SCANNER_PARSE_ERROR
  "Problem parsing file $0 at $1$2",
  // SNAPPY_DECOMPRESS_INVALID_BLOCK_SIZE
  "Decompressor: block size is too big.  Data is likely corrupt. Size: $0",
  // SNAPPY_DECOMPRESS_INVALID_COMPRESSED_LENGTH
  "Decompressor: invalid compressed length.  Data is likely corrupt.",
  // SNAPPY_DECOMPRESS_UNCOMPRESSED_LENGTH_FAILED
  "Snappy: GetUncompressedLength failed",
  // SNAPPY_DECOMPRESS_RAW_UNCOMPRESS_FAILED
  "SnappyBlock: RawUncompress failed",
  // SNAPPY_DECOMPRESS_DECOMPRESS_SIZE_INCORRECT
  "Snappy: Decompressed size is not correct.",
  // HDFS_SCAN_NODE_UNKNOWN_DISK
  "Unknown disk id.  This will negatively affect performance. Check your hdfs settings to enable block location metadata.",
  // FRAGMENT_EXECUTOR
  "Reserved resource size ($0) is larger than query mem limit ($1), and will be restricted to $1. Configure the reservation size by setting RM_INITIAL_MEM.",
  // PARTITIONED_HASH_JOIN_MAX_PARTITION_DEPTH
  "Cannot perform join at hash join node with id $0. The input data was partitioned the maximum number of $1 times. This could mean there is significant skew in the data or the memory limit is set too low.",
  // PARTITIONED_AGG_MAX_PARTITION_DEPTH
  "Cannot perform aggregation at hash aggregation node with id $0. The input data was partitioned the maximum number of $1 times. This could mean there is significant skew in the data or the memory limit is set too low.",
  // MISSING_BUILTIN
  "Builtin '$0' with symbol '$1' does not exist. Verify that all your impalads are the same version.",
  // RPC_GENERAL_ERROR
  "RPC Error: $0",
  // RPC_TIMEOUT
  "RPC timed out",
  // UDF_VERIFY_FAILED
  "Failed to verify function $0 from LLVM module $1, see log for more details.",
  // PARQUET_CORRUPT_VALUE
  "File $0 corrupt. RLE level data bytes = $1",
  // AVRO_DECIMAL_RESOLUTION_ERROR
  "Column '$0' has conflicting Avro decimal types. Table schema $1: $2, file schema $1: $3",
  // AVRO_DECIMAL_METADATA_MISMATCH
  "Column '$0' has conflicting Avro decimal types. Declared $1: $2, $1 in table's Avro schema: $3",
  // AVRO_SCHEMA_RESOLUTION_ERROR
  "Unresolvable types for column '$0': table type: $1, file type: $2",
  // AVRO_SCHEMA_METADATA_MISMATCH
  "Unresolvable types for column '$0': declared column type: $1, table's Avro schema type: $2",
  // AVRO_UNSUPPORTED_DEFAULT_VALUE
  "Field $0 is missing from file and default values of type $1 are not yet supported.",
  // AVRO_MISSING_FIELD
  "Inconsistent table metadata. Mismatch between column definition and Avro schema: cannot read field $0 because there are only $1 fields.",
  // AVRO_MISSING_DEFAULT
  "Field $0 is missing from file and does not have a default value.",
  // AVRO_NULLABILITY_MISMATCH
  "Field $0 is nullable in the file schema but not the table schema.",
  // AVRO_NOT_A_RECORD
  "Inconsistent table metadata. Field $0 is not a record in the Avro schema.",
  // PARQUET_DEF_LEVEL_ERROR
  "Could not read definition level, even though metadata states there are $0 values remaining in data page. file=$1",
  // PARQUET_NUM_COL_VALS_ERROR
  "Mismatched number of values in column index $0 ($1 vs. $2). file=$3",
  // PARQUET_DICT_DECODE_FAILURE
  "Failed to decode dictionary-encoded value. file=$0",
  // SSL_PASSWORD_CMD_FAILED
  "SSL private-key password command ('$0') failed with error: $1",
  // SSL_CERTIFICATE_PATH_BLANK
  "The SSL certificate path is blank",
  // SSL_PRIVATE_KEY_PATH_BLANK
  "The SSL private key path is blank",
  // SSL_CERTIFICATE_NOT_FOUND
  "The SSL certificate file does not exist at path $0",
  // SSL_PRIVATE_KEY_NOT_FOUND
  "The SSL private key file does not exist at path $0",
  // SSL_SOCKET_CREATION_FAILED
  "SSL socket creation failed: $0",
  // MEM_ALLOC_FAILED
  "Memory allocation of $0 bytes failed",
  // PARQUET_REP_LEVEL_ERROR
  "Could not read repetition level, even though metadata states there are $0 values remaining in data page. file=$1",
  // PARQUET_UNRECOGNIZED_SCHEMA
  "File '$0' has an incompatible Parquet schema for column '$1'. Column type: $2, Parquet schema:\n$3",
  // COLLECTION_ALLOC_FAILED
  "Failed to allocate $0 bytes for collection '$1'.\nCurrent buffer size: $2 num tuples: $3.",
  // TMP_DEVICE_BLACKLISTED
  "Temporary device for directory $0 is blacklisted from a previous error and cannot be used.",
  // TMP_FILE_BLACKLISTED
  "Temporary file $0 is blacklisted from a previous error and cannot be expanded.",
  // RPC_CLIENT_CONNECT_FAILURE
  "RPC client failed to connect: $0",
  // STALE_METADATA_FILE_TOO_SHORT
  "Metadata for file '$0' appears stale. Try running \"refresh $1\" to reload the file metadata.",
  // PARQUET_BAD_VERSION_NUMBER
  "File '$0' has an invalid version number: $1\nThis could be due to stale metadata. Try running \"refresh $2\".",
  // SCANNER_INCOMPLETE_READ
  "Tried to read $0 bytes but could only read $1 bytes. This may indicate data file corruption. (file $2, byte offset: $3)",
  // SCANNER_INVALID_READ
  "Invalid read of $0 bytes. This may indicate data file corruption. (file $1, byte offset: $2)",
  // AVRO_BAD_VERSION_HEADER
  "File '$0' has an invalid version header: $1\nMake sure the file is an Avro data file.",
  // UDF_MEM_LIMIT_EXCEEDED
  "$0's allocations exceeded memory limits.",
  // BTS_BLOCK_OVERFLOW
  "Cannot process row that is bigger than the IO size (row_size=$0, null_indicators_size=$1). To run this query, increase the IO size (--read_size option).",
  // COMPRESSED_FILE_MULTIPLE_BLOCKS
  "For better performance, snappy-, gzip-, and bzip-compressed files should not be split into multiple HDFS blocks. file=$0 offset $1",
  // COMPRESSED_FILE_BLOCK_CORRUPTED
  "$0 Data error, likely data corrupted in this block.",
  // COMPRESSED_FILE_DECOMPRESSOR_ERROR
  "$0 Decompressor error at $1, code=$2",
  // COMPRESSED_FILE_DECOMPRESSOR_NO_PROGRESS
  "Decompression failed to make progress, but end of input is not reached. File appears corrupted. file=$0",
  // COMPRESSED_FILE_TRUNCATED
  "Unexpected end of compressed file. File may be truncated. file=$0",
  // DATASTREAM_SENDER_TIMEOUT
  "Sender timed out waiting for receiver fragment instance: $0",
  // KUDU_IMPALA_TYPE_MISSING
  "Kudu type $0 is not available in Impala.",
  // IMPALA_KUDU_TYPE_MISSING
  "Impala type $0 is not available in Kudu.",
  // KUDU_NOT_SUPPORTED_ON_OS
  "Kudu is not supported on this operating system.",
  // KUDU_NOT_ENABLED
  "Kudu features are disabled by the startup flag --disable_kudu.",
  // PARTITIONED_HASH_JOIN_REPARTITION_FAILS
  "Cannot perform hash join at node with id $0. Repartitioning did not reduce the size of a spilled partition. Repartitioning level $1. Number of rows $2.",
  // PARTITIONED_AGG_REPARTITION_FAILS
  "Cannot perform aggregation at node with id $0. Repartitioning did not reduce the size of a spilled partition. Repartitioning level $1. Number of rows $2."
]